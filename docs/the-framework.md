# The Agent-Native Firm

**A framework for building companies where AI does the work and humans do the thinking.**

---

## Why This Document Exists

In 1937, Ronald Coase asked the most fundamental question in economics: why do companies exist at all? His answer — transaction costs — won him a Nobel Prize and defined how we think about business for almost a century.

AI agents are about to make that answer obsolete.

When the cost of finding, hiring, coordinating, and managing people was high, it made sense to bring them inside a firm. When AI collapses those costs to near zero, the optimal shape of a company changes fundamentally. Most people haven't connected these dots yet.

This document is a framework for the company that emerges on the other side.

---

## Part I: The Theoretical Foundation

### Why Companies Exist (And Why They're About to Shrink)

**Coase (1937)** proved that firms exist because market transactions have friction — finding suppliers, negotiating contracts, enforcing agreements, monitoring quality. When those costs exceed the cost of doing things in-house, you hire people.

**Williamson (1985)** extended this: the more specialized your assets and the more uncertain your environment, the more you need hierarchical control (a company) rather than market transactions (freelancers and vendors).

AI agents collapse both sides of this equation:

- **Search and information costs approach zero.** An agent evaluates options, synthesizes data, and surfaces recommendations in seconds. No procurement department needed.
- **Coordination costs disappear.** Agents don't need meetings, don't misunderstand briefs, don't have conflicting priorities. They read the specification and execute.
- **Monitoring is automatic.** Every agent action is logged. The principal-agent problem — where employees may shirk or pursue self-interest — is structurally eliminated for agent-performed work.
- **Contracting overhead vanishes.** No HR process, no salary negotiation, no benefits package, no office lease. An agent spins up, executes, and shuts down.

**The Coasean implication is radical:** If transaction costs defined the boundary of the firm, and AI collapses those costs, the firm contracts to its irreducible core. That core is whatever cannot be transacted in a market or delegated to an agent.

What remains is taste, judgment, relationships, and brand.

### What Only Humans Can Do

**Michael Polanyi (1966)** articulated this precisely: "We can know more than we can tell." He called it tacit knowledge — the kind of understanding that can't be written down, codified into rules, or transmitted through language.

A pianist doesn't think about individual finger movements. A surgeon doesn't consciously process each tissue layer. A founder who says "this feels cheap" or "that audience isn't ready" is exercising tacit knowledge built from years of pattern recognition, cultural immersion, and lived experience.

**David Autor (2014)** formalized this for economics as the Polanyi Paradox: many tasks that are trivial for humans are hard for computers precisely because they rely on tacit knowledge. AI complements rather than replaces workers whose jobs require this kind of understanding.

This maps precisely to the division of labor in the agent-native firm:

**What agents handle (explicit knowledge):**
- Write code that conforms to specifications
- Apply design systems consistently
- Process, structure, and enrich data
- Follow documented patterns and conventions
- Generate content within defined parameters
- Research, analyze, and synthesize information

**What humans provide (tacit knowledge):**
- **Taste** — "This feels premium" vs. "this feels like AI slop." No rule system can fully capture this judgment. The human knows the audience at a level that transcends articulation.
- **Market sensing** — Which opportunity is real vs. which is noise. Pattern recognition from experience, intuition from cultural immersion.
- **Relational judgment** — Who to trust, which partnerships to pursue, when a conversation is going well. Polanyi called this "connoisseurship."
- **Aesthetic integration** — Seeing how all the pieces fit. Not evaluating components individually but perceiving the gestalt. "This flows" or "something is off" are tacit judgments integrating hundreds of variables simultaneously.

**Harry Collins (2010)** refined this further with three types of tacit knowledge: relational (could be made explicit but hasn't been), somatic (embodied in the body), and collective (embedded in social groups). AI can potentially handle the first. It struggles with the second. It cannot replicate the third.

The brand ambassador role — being embedded in a community, building trust face-to-face — is fundamentally about collective tacit knowledge.

### The Knowledge Company

**Grant (1996)** and **Nonaka & Takeuchi (1995)** argued that firms exist primarily as knowledge-creating entities. The firm's purpose isn't to minimize transaction costs — it's to integrate the specialized knowledge of many people into productive activity.

In the traditional knowledge-based view, a firm integrates knowledge held by engineers, marketers, designers, and salespeople. In the agent-native firm, the knowledge architecture inverts:

- **AI agents hold and process explicit knowledge.** Every codifiable fact, documented process, and data pattern is their domain. They store, retrieve, combine, and apply explicit knowledge at superhuman speed.
- **Humans hold irreducible tacit knowledge.** The seed intuitions, the aesthetic standards, the relational intelligence.
- **The knowledge spiral accelerates.** Nonaka's SECI model — Socialization, Externalization, Combination, Internalization — still operates, but faster. The human externalizes tacit knowledge into agent instructions. Agents combine explicit knowledge at scale. The human internalizes the output and develops new intuitions. The one dimension that stays fully human: socialization — tacit-to-tacit transfer between people. This is why the brand ambassador role exists.

The firm becomes a **knowledge engine** where human intuition seeds the process, agents amplify it through explicit processing, and the outputs generate new tacit understanding in the human. A virtuous cycle.

The knowledge itself is the asset. Everything else — the agents, the infrastructure, the product — is distribution.

### The Prediction-Judgment Split

**Agrawal, Gans & Goldfarb (2022)** provide the most rigorous economic framing of how AI reshapes organizations. Their core argument: AI is fundamentally a prediction technology. It decouples prediction from judgment — two things that have historically been bundled together inside human decision-making.

- **Prediction** — What will happen? What do users want? What content will perform? Which market is growing? AI is increasingly better at this than humans.
- **Judgment** — What to do about those predictions? What risks to accept? What values to encode? What trade-offs to make? This remains human.

Traditional firms bundle prediction and judgment in the same roles. A marketing director both predicts what will resonate and judges what's worth doing. A product manager both predicts user behavior and decides what to build. This bundling means you're paying specialist salaries for the prediction component, which AI can now handle.

The agent-native firm unbundles prediction from judgment cleanly:
- Agents handle prediction-intensive tasks (market research, data analysis, A/B testing, pattern recognition in user behavior)
- Humans handle judgment-intensive tasks (brand positioning, quality standards, ethical boundaries, strategic direction)

Agrawal et al. call this "system-level disruption" — not improving individual tasks but enabling entirely new decision architectures. The agent-native firm is a new decision architecture.

### The Centaur Model

**Ethan Mollick (2024)** at Wharton provides the most empirically grounded research on human-AI collaboration in organizations. His key finding: the best outcomes come from what he calls "centaur" models — where humans and AI divide tasks based on comparative advantage — or "cyborg" models — where they work jointly on the same tasks.

The agent-native firm is centaur for execution (clear division: humans direct, agents execute) and cyborg for direction (the human thinks alongside AI, using agents to research, prototype, and test ideas before committing).

Mollick's critical caution: people systematically underestimate how much judgment is required in tasks that appear routine, and overestimate how much AI can handle without oversight. The implication for the framework: don't assume agents can run on autopilot. The human must stay close enough to catch what the agents miss. Abdication is not delegation.

### The Counter-Thesis: Technology Doesn't Automatically Create Value

**Acemoglu & Johnson (2023)** in *Power and Progress* provide the strongest counterargument to the entire AI-native thesis. Their argument, drawing on 1,000 years of technological change: technology does not automatically benefit its deployers or society. It depends on institutional choices, power dynamics, and whether the technology creates genuine new value or merely redistributes existing value.

They warn specifically against "so-so automation" — technology that replaces human workers without creating commensurate new capabilities. The risk for the agent-native firm: building products that are "good enough" but lack the depth, nuance, and human understanding that intensive human involvement would provide.

This is an honest challenge. The mitigation is the quality filter — the human's taste function must be genuinely excellent, not merely present. A mediocre human directing excellent agents produces mediocre output at scale. The framework only works when the human contribution is genuinely irreplaceable.

---

## Part II: The Architecture

### The Three Functions

Every company, stripped to its essence, performs three functions: it **builds** something, it **operates** something, and it **sells** something. The agent-native firm redistributes who does what.

```
┌─────────────────────────────────────────────────────────┐
│                                                         │
│   DIRECTION          EXECUTION          INTERFACE       │
│   (Human)            (Agents)           (Human)         │
│                                                         │
│   What to build      Build it           Who sees it     │
│   What standard      Maintain it        Who trusts it   │
│   What matters       Improve it         Who buys it     │
│                                                         │
└─────────────────────────────────────────────────────────┘
```

**Direction** is the founder function. Deciding what to build, what quality standard to maintain, which market to enter, when to pivot. This is pure tacit knowledge — taste, judgment, vision. It cannot be delegated.

**Execution** is the agent function. Writing code, processing data, building products, maintaining systems, enriching content, iterating on designs. This is explicit knowledge work performed at near-zero marginal cost.

**Interface** is the ambassador function. Being the human face. Building relationships with users, partners, press. Creating trust that an AI system alone cannot generate. Gathering the feedback that feeds back into Direction.

The radical insight: in a traditional company, you need 50-200 people because execution is labor-intensive. In this model, execution is handled by agents. The human roles collapse to Direction (1 person) and Interface (1 person). These can be the same person, or different people.

### The Specialist Model: Buy the Foundation, Own the Iteration

This is where the marketing example matters, and it generalizes.

For any specialized function — brand identity, legal structure, financial modeling, UX research — there's a pattern:

**Phase 1: Foundational work (hire a specialist)**
- Hire a brand designer to create the identity system, voice guidelines, visual language
- Hire a lawyer to set up the corporate structure, draft standard contracts
- Hire a UX researcher to conduct initial user interviews, define personas

**Phase 2: Codification (capture the knowledge)**
- The specialist's output gets codified into documents, guidelines, design systems
- This explicit knowledge becomes part of the agent's instruction set
- The agents can now execute within these guidelines at scale

**Phase 3: Ongoing operation (agents + occasional consultation)**
- Agents apply the guidelines to every new design, every new piece of content, every new decision
- The specialist is consulted periodically when the guidelines need updating or a novel situation arises
- Cost drops from a full-time salary to occasional consulting fees

This inverts the traditional hiring model. You don't build a team. You **buy expertise, codify it, and let agents operationalize it.** The specialist's value isn't in their ongoing labor — it's in the knowledge they transfer to the system.

**Prahalad & Hamel (1990)** would recognize this as redefining core competency. The core competency isn't design, or engineering, or marketing. It's **the ability to identify what expertise to acquire, codify it effectively, and direct agents to apply it.** That meta-competency transfers across domains.

### The Scarce Resource: Attention

In traditional firms, the scarce resource is capital. Decisions revolve around where to allocate money.

In the agent-native firm, building is cheap. The scarce resource is the human's **attention** — their time, judgment, and creative energy.

This parallels **Berkshire Hathaway's** model. Warren Buffett ran a $700B+ company with ~30 people at headquarters. His only function was capital allocation — deciding where to deploy resources for maximum return. Everything else was decentralized to subsidiary managers.

Your model mirrors this but replaces capital allocation with **attention allocation**. Each product in the portfolio needs the human's taste and direction. The human, like Buffett, must decide where to deploy the scarce resource (attention) for maximum return.

This means the framework has a natural scaling limit: one human can meaningfully direct perhaps 2-5 products simultaneously. Beyond that, you need more humans with taste — which is what the brand ambassador model enables. Each product gets its own ambassador who provides direction and interface, while sharing the agent infrastructure.

---

## Part III: The Economics

### Zero Marginal Cost Production

**Rifkin (2014)** predicted that capitalism would generate an internal contradiction: efficiency would push marginal costs toward zero, undermining the profit mechanism. He was writing about distributing digital goods. AI extends his thesis to **creating** them.

Before AI:
- Marginal cost of distributing software: ~zero
- Marginal cost of creating software: high (developer salaries, design time, QA, management)

With AI agents:
- Marginal cost of distributing software: ~zero
- Marginal cost of creating software: approaching zero (API costs per feature)

The first product in the portfolio is the most expensive — you're establishing the playbook, calibrating the agents, building the brand. Each subsequent product leverages the same human taste, operational patterns, and agent capabilities at dramatically lower cost.

**What this makes viable:** Markets too small for traditional companies become profitable. A niche community of 10,000 people can sustain a premium product when the cost of building and maintaining it is measured in hundreds of dollars per month rather than millions per year.

**Constellation Software** has already proven a version of this thesis in traditional business. Mark Leonard built a $70B+ company by acquiring 800+ small vertical-market software products — niche tools for golf courses, transit systems, libraries. Markets too boring and small for PE or VC. Each one generates reliable cash flow with minimal maintenance.

The agent-native version replaces "acquire" with "build." Same strategy — target underserved niches — but building is cheaper than acquiring. Constellation pays 0.5-1.5x revenue. This model builds for the cost of API calls and human time.

### The Real Cost Structure

| Function | Traditional Company | Agent-Native Firm |
|----------|-------------------|-------------------|
| Engineering | $500K-2M/year | $2-10K/year (API + infra) |
| Design | $200-500K/year | Codified in agent instructions |
| Content | $100-300K/year | Agent-generated, human-curated |
| Operations | $300K-1M/year | 1 human + agents |
| Marketing | $200K-1M/year | 1 brand ambassador + agents |
| Management | $300K-1M/year | Not needed |
| **Total** | **$1.5-5M/year** | **$100-300K/year** |

This isn't theoretical. It's the actual operating cost of building a production-quality application with AI agents today.

### Antifragility

**Taleb (2012)** introduced a concept beyond resilience: **antifragility** — systems that gain from disorder. The agent-native firm is structurally antifragile:

**Convex payoff profile.** Limited downside (low fixed costs — a bad month costs almost nothing), unlimited upside (a successful product can generate significant revenue). This is Taleb's barbell strategy applied to firm structure.

**Optionality through cheap experiments.** Launching a new product costs days of agent time, not months of team coordination. This enables many small bets. Most fail. A few succeed. The portfolio benefits from this optionality.

**Benefits from volatility.** Market disruptions that destroy traditional firms — a pandemic, a regulatory change, a technology shift — can benefit the agent-native firm because it can pivot at near-zero cost. No employees to lay off, no leases to break, no specialized infrastructure to write off.

**Via negativa.** Taleb emphasized that antifragility often comes from removal. Don't hire people you'll need to fire. Don't sign leases you'll need to break. Don't build proprietary infrastructure you'll need to maintain. Don't accumulate the coordination overhead of a large team.

---

## Part IV: The Portfolio Model

### LVMH as Template

**Bernard Arnault** built the closest analog to what this framework enables: a portfolio of 75+ brands, each maintaining its own creative identity, sharing back-office functions. The center provides capital, strategic direction, and quality control. Individual brands retain creative autonomy.

The LVMH insight: **the brand IS the product.** Not the handbag, not the perfume, not the champagne. The meaning, the aspiration, the cultural positioning. The creative director of each brand is the irreducible human element — the taste function.

Map this to the agent-native portfolio:

| LVMH | Agent-Native Portfolio |
|------|----------------------|
| Brand (Louis Vuitton, Dior) | Product vertical (travel, wellness, education) |
| Creative Director | Brand ambassador / domain lead |
| Craftspeople and artisans | AI agents |
| Shared supply chain | Shared agent infrastructure and design system |
| Brand equity | Knowledge base + community trust |

The lesson that matters most: **brand equity compounds, but one cheap product can destroy it.** LVMH maintains quality control at the expense of growth. The agent-native portfolio must do the same. The human's primary function is quality control — maintaining the standard that makes the brand worth trusting.

### The Replication Playbook

Each new vertical follows the same pattern:

1. **Identify a knowledge gap.** Where do people need structured, trustworthy information that doesn't exist in clean form? Solo women travelers. Immigrant families navigating a new country. First-generation college students. Chronic illness communities. Niche hobbyist markets.

2. **Acquire the foundational expertise.** Hire specialists to establish the brand identity, research the domain, define the information architecture. Codify everything.

3. **Deploy agent infrastructure.** Same operational model, same agent workflows, same quality standards. The framework is the constant, the domain is the variable.

4. **Install a brand ambassador.** One person who lives in that world. Who has the tacit knowledge of the community. Who can build trust face-to-face.

5. **Compound the knowledge.** Let the product generate data, let agents process it, let the knowledge base grow. Each user interaction makes the product more valuable.

6. **Reach sustainability.** Small, focused products serving specific communities don't need millions of users. They need enough users to sustain the (very low) cost structure, and enough knowledge to create a moat.

---

## Part V: What Could Go Wrong

A framework that doesn't account for its own failure modes isn't a framework — it's a sales pitch. These are the real risks, drawn from the same literature that supports the model.

### 1. Single Point of Failure (Taleb)

Every "one-person company" case study is also a story of extreme fragility. The human is the single point of failure. Burnout, health issues, loss of motivation — any of these disables the entire system. There is no redundancy in the human layer.

**Mitigation:** The portfolio model distributes this risk across brand ambassadors. The founder provides the meta-framework; individual products have their own human leads. But this introduces coordination costs that partially offset the model's advantages.

### 2. Taste Calcification (Leonard-Barton 1992)

Core competencies can become "core rigidities." The founder's taste gets codified into agent instructions and design systems. Over time, it calcifies. The firm loses the ability to see differently. What was once innovative becomes formulaic.

**Mitigation:** Periodic external review. New brand ambassadors with fresh perspectives. Deliberate exposure to unfamiliar markets and aesthetics.

### 3. Platform Dependency (Williamson)

The model depends on AI API availability. A catastrophic failure at Anthropic or OpenAI — regulatory shutdown, pricing change, capability regression — is a tail risk that could disable the entire firm. Taleb would call this a "hidden fragility."

**Mitigation:** Avoid lock-in to any single AI provider. Keep agent instructions portable across models. Maintain the ability to rebuild on alternative infrastructure. This is the modern equivalent of Williamson's asset specificity problem — don't let your firm-specific knowledge become provider-specific.

### 4. The Distribution Bottleneck (Ben Thompson)

**Ben Thompson (Stratechery)** argued that when AI handles production, the bottleneck shifts to distribution, trust, and relationships — fundamentally human activities that don't scale with AI. Building the product becomes easy. Getting people to use it remains hard.

**Mitigation:** This is exactly what the brand ambassador role addresses. But it means the model's constraint isn't building — it's distribution. Marketing, community building, partnerships — these remain labor-intensive and human-dependent. The framework accounts for this by keeping the human focused on interface rather than execution.

### 5. Quality Perception

If AI-built products are perceived as "AI slop," the zero-marginal-cost advantage becomes a disadvantage. The market floods with low-quality AI-generated products, and consumers develop a bias against anything that feels algorithmic.

**Mitigation:** The human curation layer is the defense. But this requires the human to maintain genuine taste — which means staying close to the audience, not hiding behind agents. The brand ambassador must be authentically embedded in the community, not performing authenticity from a distance.

### 6. The Race to the Bottom (Rifkin's critics)

If marginal costs approach zero for everyone, competition drives prices to zero. Anyone can spin up agents and build a competing product in days. What's the moat?

**Mitigation:** Three moats that compound over time:
- **Knowledge base** — data that accumulates with use and becomes harder to replicate
- **Community network effects** — relationships between users that create switching costs
- **Brand trust** — accumulated credibility that takes years to build and seconds to destroy

### 7. Judgment Doesn't Diversify

Berkshire Hathaway works because Buffett is arguably the greatest capital allocator in history. The model is deeply dependent on the quality of the center's judgment. If the founder's taste is mediocre, the entire portfolio suffers simultaneously. There is no diversification of judgment.

**Byrne Hobart (The Diff)** argued that the real unlock is not "one person" but "small team with AI leverage." The optimal size may be 3-7 people, not 1. Judgment needs to be stress-tested. A single perspective is inherently limited.

**Mitigation:** Honest self-assessment. External advisors. Brand ambassadors who push back. Building in feedback loops that surface bad decisions before they propagate across the portfolio.

---

## Part VI: The Human-Agent Division of Labor

This is the operational core. For every function a company performs, the question is: what does the human do, and what do agents do?

### Building (Product Development)

| Human | Agent |
|-------|-------|
| Define the vision ("Airbnb meets Notion meets Lonely Planet") | Research existing solutions and patterns |
| Set the quality standard ("no AI slop, no generic icons") | Implement designs and write code |
| Review and reject ("too heavy, make it float") | Iterate based on feedback |
| Approve final output | Maintain, test, deploy |

**The human provides the "what" and "why." Agents figure out the "how."**

### Brand and Identity

| Human | Agent |
|-------|-------|
| Hire specialist to create initial brand identity | Apply brand guidelines to every touchpoint |
| Define voice, tone, positioning | Generate content within those parameters |
| Approve/reject brand expressions | Maintain consistency at scale |
| Evolve the brand over time | Update all assets when direction changes |

**Phase 1 is human-intensive. Phase 2+ is agent-dominant with human oversight.**

### Marketing and Distribution

| Human | Agent |
|-------|-------|
| Build relationships (press, partners, influencers) | Prepare materials, research, draft outreach |
| Show up in person (events, communities, social) | Schedule, optimize, track analytics |
| Create the narrative | Distribute the narrative across channels |
| Read the room | Process the data |

**This is the function that stays most human. Distribution is relationship-dependent.**

### Operations

| Human | Agent |
|-------|-------|
| Set policies and standards | Execute operations against those standards |
| Handle edge cases and judgment calls | Handle routine operations at scale |
| Manage vendor relationships | Process data, maintain systems, monitor health |
| Decide what to measure | Measure it, report it, flag anomalies |

### Legal and Finance

| Human | Agent |
|-------|-------|
| Hire specialist for foundational work (incorporation, contracts) | Apply templates, track compliance, generate reports |
| Make strategic decisions (pricing, fundraising) | Model scenarios, process transactions |
| Sign contracts, make commitments | Draft, review, organize |
| Consult specialists for novel situations | Handle routine matters within established parameters |

### Knowledge Management

| Human | Agent |
|-------|-------|
| Decide what knowledge matters | Collect, structure, and enrich it |
| Curate for quality and relevance | Process at scale |
| Identify gaps in the knowledge base | Fill gaps through research and synthesis |
| Define the editorial standard | Apply it consistently |

---

## Part VII: What This Is Called

Several frameworks approach this from different angles. None fully captures it.

- **Sam Altman's "one-person billion dollar company"** — captures the leverage but misses the brand ambassador role and the portfolio model
- **Solo Capitalist (Nikhil Basu Trivedi)** — captures the minimal-structure thesis but was about VC, not operating companies
- **Pieter Levels' solo-developer model** — captures the execution but lacks the theoretical framework and brand discipline
- **LVMH's portfolio model** — captures the brand and taste-driven structure but uses traditional human labor for execution

This framework synthesizes these into something more specific:

**The Agent-Native Firm** — a company designed from the ground up for AI execution, where:
1. The knowledge base is the core asset
2. AI agents handle all explicit knowledge work (building, operating, maintaining)
3. Humans provide the irreducible tacit inputs (taste, judgment, relationships)
4. A brand ambassador is the public interface that creates trust
5. The model is designed to be replicated across domains as a portfolio
6. Specialists are hired for foundational work, then their expertise is codified and operationalized by agents
7. The scarce resource is human attention, not capital

It is rooted in:
- Coase and Williamson on why firms exist and how AI changes the calculus
- Polanyi and Collins on what knowledge remains irreducibly human
- Grant and Nonaka on firms as knowledge-creating entities
- Taleb on building antifragile structures with convex payoffs
- Prahalad and Hamel on core competency as a meta-capability
- The Berkshire/LVMH/Constellation models on portfolio leverage with minimal central overhead

---

## Part VIII: First Principles

If this framework works, it works because of a few things that are true about the world right now:

1. **AI can execute explicit knowledge work at near-zero marginal cost.** This is empirically true today and the cost is decreasing.

2. **Taste and judgment remain scarce.** The supply of AI-generated content is exploding. The ability to distinguish good from bad, to curate, to maintain a standard — this is becoming more valuable, not less.

3. **Trust is still human-to-human.** People buy from people they trust. No amount of AI polish substitutes for a human who shows up, builds relationships, and stands behind the product.

4. **Information compounds.** A knowledge base that grows with use becomes harder to replicate over time. This is the moat that protects against commoditization.

5. **Small, focused markets are defensible.** Generalist platforms serve everyone poorly. Niche products serve specific communities well. The economics of the agent-native firm make niche markets viable that were previously too small to address.

6. **The framework is the product.** The first company built this way is a proof of concept. The framework itself — the operational model, the human-agent division of labor, the replication playbook — is what scales.

---

## References

### Foundational Theory
- Coase, R.H. (1937). "The Nature of the Firm." *Economica*, 4(16), 386-405.
- Williamson, O.E. (1985). *The Economic Institutions of Capitalism*. Free Press.
- Polanyi, M. (1966). *The Tacit Dimension*. University of Chicago Press.
- Penrose, E.T. (1959). *The Theory of the Growth of the Firm*. Oxford University Press.
- Jensen, M. & Meckling, W. (1976). "Theory of the Firm." *Journal of Financial Economics*, 3(4), 305-360.

### Knowledge and Competency
- Grant, R.M. (1996). "Toward a Knowledge-Based Theory of the Firm." *Strategic Management Journal*, 17(S2), 109-122.
- Nonaka, I. & Takeuchi, H. (1995). *The Knowledge-Creating Company*. Oxford University Press.
- Prahalad, C.K. & Hamel, G. (1990). "The Core Competence of the Corporation." *Harvard Business Review*, 68(3), 79-91.
- Collins, H. (2010). *Tacit and Explicit Knowledge*. University of Chicago Press.
- Kogut, B. & Zander, U. (1992). "Knowledge of the Firm." *Organization Science*, 3(3), 383-397.
- Leonard-Barton, D. (1992). "Core Capabilities and Core Rigidities." *Strategic Management Journal*, 13(S1), 111-125.
- Teece, D.J., Pisano, G. & Shuen, A. (1997). "Dynamic Capabilities and Strategic Management." *Strategic Management Journal*, 18(7), 509-533.

### Platform and Network Economics
- Rochet, J.C. & Tirole, J. (2003). "Platform Competition in Two-Sided Markets." *JEEA*, 1(4), 990-1029.
- Parker, G., Van Alstyne, M. & Choudary, S.P. (2016). *Platform Revolution*. W.W. Norton.
- Shapiro, C. & Varian, H. (1999). *Information Rules*. Harvard Business School Press.
- Evans, D. & Schmalensee, R. (2016). *Matchmakers*. Harvard Business Review Press.

### Digital Economics and Zero Marginal Cost
- Rifkin, J. (2014). *The Zero Marginal Cost Society*. Palgrave Macmillan.
- Brynjolfsson, E. & McAfee, A. (2014). *The Second Machine Age*. W.W. Norton.

### Risk and Antifragility
- Taleb, N.N. (2012). *Antifragile: Things That Gain from Disorder*. Random House.
- Taleb, N.N. (2007). *The Black Swan*. Random House.

### AI and Firm Structure
- Agrawal, A., Gans, J. & Goldfarb, A. (2022). *Power and Prediction*. Harvard Business Press.
- Iansiti, M. & Lakhani, K. (2020). *Competing in the Age of AI*. Harvard Business School Press.
- Mollick, E. (2024). *Co-Intelligence*. Portfolio/Penguin.
- Amodei, D. (2024). "Machines of Loving Grace." Essay.
- Autor, D. (2014). "Polanyi's Paradox and the Shape of Employment Growth." *NBER Working Paper 20485*.

### Critical Perspectives
- Acemoglu, D. & Johnson, S. (2023). *Power and Progress*. PublicAffairs.
- Mazzucato, M. (2018). *The Value of Everything*. PublicAffairs.

---

*This is a living document. The framework is being tested in practice. What survives contact with reality will be refined. What doesn't will be cut.*
